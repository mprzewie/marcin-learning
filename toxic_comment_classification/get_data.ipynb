{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(text, label):\n",
    "    feature = {\n",
    "        \"text\": _bytes_feature(text),\n",
    "        \"label\": _int64_feature(label)\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def tf_serialize_example(text, label):\n",
    "    tf_string = tf.py_function(\n",
    "    serialize,\n",
    "    (text, label),  # pass these args to the above function.\n",
    "    tf.string)      # the return type is `tf.string`.\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeerak hatespeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/hatespeech/NAACL_SRW_2016.csv\")\n",
    "df[\"racism\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supremacist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10944it [00:00, 29155.12it/s]\n"
     ]
    }
   ],
   "source": [
    "comments = dict()\n",
    "for p in tqdm(Path(\"data/hate-speech-dataset/all_files/\").glob(\"*.txt\")):\n",
    "    with p.open(\"r\") as f:\n",
    "        t = f.read()\n",
    "    comments[p.stem] = t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12834217_4</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_5</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10939</th>\n",
       "      <td>33676864_5</td>\n",
       "      <td>734541</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10940</th>\n",
       "      <td>33677019_1</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10941</th>\n",
       "      <td>33677019_2</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10942</th>\n",
       "      <td>33677053_1</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10943</th>\n",
       "      <td>33677053_2</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10944 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  user_id  subforum_id  num_contexts   label\n",
       "0      12834217_1   572066         1346             0  noHate\n",
       "1      12834217_2   572066         1346             0  noHate\n",
       "2      12834217_3   572066         1346             0  noHate\n",
       "3      12834217_4   572066         1346             0    hate\n",
       "4      12834217_5   572066         1346             0  noHate\n",
       "...           ...      ...          ...           ...     ...\n",
       "10939  33676864_5   734541         1388             0  noHate\n",
       "10940  33677019_1   735154         1388             0  noHate\n",
       "10941  33677019_2   735154         1388             0  noHate\n",
       "10942  33677053_1   572266         1388             0    hate\n",
       "10943  33677053_2   572266         1388             0  noHate\n",
       "\n",
       "[10944 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/hate-speech-dataset/annotations_metadata.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:8000]\n",
    "df_val = df[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_comments(df):\n",
    "    def iterator():\n",
    "        for i, row in df.iterrows():\n",
    "            comment = comments[row.file_id]\n",
    "            label = int(row.label == \"hate\")\n",
    "            yield (comment, label)\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_generator(iter_comments(df_train), output_types=(tf.string, tf.int32))\n",
    "ds_val = tf.data.Dataset.from_generator(iter_comments(df_val), output_types=(tf.string, tf.int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_train = tf.data.experimental.TFRecordWriter(\"supremacists_train.tfrecord\")\n",
    "writer_train.write(ds_train.map(tf_serialize_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_val = tf.data.experimental.TFRecordWriter(\"supremacists_val.tfrecord\")\n",
    "writer_val.write(ds_val.map(tf_serialize_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mprzewie/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in ds_val:\n",
    "    text = x.numpy().decode()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Thats what I tried to tell those nuts over on the health a fitness thread and was attacked for it in post ????\n",
      "Augmented Text:\n",
      "Thats what I tried to tell those nuts terminated on the health a fitness yarn and be attacked for it in mail service ? ? ? ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Thats what I tried to tell those nuts over on the health a fitness thread and was attacked for it in post ????\n",
      "Augmented Text:\n",
      "Thats what I tried to tell those nuts over off the health a fitness thread and was attacked for it in post ? ? ? ?\n"
     ]
    }
   ],
   "source": [
    "aug = naw.AntonymAug()\n",
    "augmented_text = aug.augment(text,)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x):\n",
    "    x = x.numpy().decode()\n",
    "    x = aug.augment(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'As of March 13th , 2014 , the booklet lack been downloaded over 18 , 300 times and counting .'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for x in ds_train.map(\n",
    "    lambda x,y: (tf.py_function(augment, [x], tf.string), y)\n",
    "):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp_personal",
   "language": "python",
   "name": "mp_personal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
