{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torchmeta.datasets.helpers import omniglot\n",
    "from torchmeta.datasets import Omniglot\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmeta.transforms import Categorical\n",
    "from dotted.utils import dot\n",
    "import hypernet as hn\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, K = 5, 5\n",
    "\n",
    "dataset = omniglot(\"data\", ways=N, shots=K, test_shots=15, meta_train=True, download=True)\n",
    "dataloader = BatchMetaDataLoader(\n",
    "    dataset, batch_size=32, num_workers=0, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task(\n",
    "    x_train, y_train, x_test, y_test, n\n",
    "):\n",
    "    for (t, x,y) in [(\"train\",x_train, y_train), (\"test\", x_test, y_test)]:\n",
    "        k = len(x) // n\n",
    "        fig, ax = plt.subplots(n, k, figsize=(1.5*k, 1.5*n))\n",
    "        for n_ in range(n):\n",
    "            for k_ in range(k):\n",
    "                i = n_*k + k_\n",
    "                ax[n_,k_].imshow(x[i].squeeze())\n",
    "                ax[n_,k_].set_title(f\"{y[i]}\")\n",
    "                ax[n_,k_].axis(\"off\")\n",
    "        fig.suptitle(t)\n",
    "        \n",
    "        \n",
    "#     print([t.shape for t in [x_train, y_train, x_test, y_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = T[\"train\"]\n",
    "x_test, y_test = T[\"test\"]\n",
    "\n",
    "plot_task(x_train[0], y_train[0], x_test[0], y_test[0], N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = hn.HyperNetwork(\n",
    "#     target_network=target_net,\n",
    "    n=N, k=K,\n",
    "    hidden_size=32\n",
    ")\n",
    "hypernet.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "h_opt = torch.optim.Adam(hypernet.parameters(), lr=4e-4)\n",
    "sum(p.numel() for p in hypernet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-overview",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_tasksets = 100\n",
    "taskset_epochs = 10\n",
    "\n",
    "for task_set_id, tasks in enumerate(tqdm(dataloader, total=n_tasksets)):\n",
    "\n",
    "    for e in range(taskset_epochs):\n",
    "        X_train, Y_train = tasks[\"train\"]\n",
    "        X_test, Y_test = tasks[\"test\"]\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = [t.to(device) for t in [X_train, X_test, Y_train, Y_test]]\n",
    "\n",
    "\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "\n",
    "        test_losses = []\n",
    "        test_accs = []\n",
    "        loss_sum = 0\n",
    "        \n",
    "\n",
    "        for t_id, (x_train, y_train, x_test, y_test) in enumerate(zip(X_train, Y_train, X_test, Y_test)):\n",
    "        \n",
    "            tn = hypernet(x_train, y_train)\n",
    "            \n",
    "            y_pred_train = tn(x_train)\n",
    "            \n",
    "            y_pred_test = tn(x_test)\n",
    "\n",
    "            loss = loss_fn(y_pred_train, y_train) + loss_fn(y_pred_test, y_test)\n",
    "            \n",
    "            loss_sum = loss_sum + loss\n",
    "            \n",
    "            train_acc = (y_pred_train.argmax(dim=1) == y_train).sum() / len(y_train)\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            train_accs.append(train_acc.item())\n",
    "            \n",
    "            test_acc = (y_pred_test.argmax(dim=1) == y_test).sum() / len(y_test)\n",
    "            test_accs.append(test_acc.item())\n",
    "            \n",
    "\n",
    "        loss_sum.backward()\n",
    "        h_opt.step()\n",
    "        h_opt.zero_grad()\n",
    "\n",
    "        print(\n",
    "            task_set_id, \n",
    "            e,\n",
    "            {\n",
    "                \"tr_l\": np.mean(train_losses).item(),\n",
    "                \"te_l\": np.mean(test_losses).item(),\n",
    "                \"tr_a\": np.mean(train_accs).item(),\n",
    "                \"te_a\": np.mean(test_accs).item()\n",
    "            } \n",
    "             )\n",
    "\n",
    "    if task_set_id == n_tasksets-1:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uj",
   "language": "python",
   "name": "uj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
